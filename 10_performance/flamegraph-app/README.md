# Interpreting flame graphs

A flame graph is a visual tool that allows us to identify â€œhot code pathsâ€ within our application. The
term â€œhot code pathâ€ is used to describe execution paths in the program that consume a relatively
large amount of time, which can indicate a bottleneck in an application.

Flame graphs provide a visualization of an applicationâ€™s call stack during execution. From this
visualization, itâ€™s possible to determine which functions are spending the most time on the CPU while
the application is running.

Here weâ€™re going to use the `0x` flame graph tool (<https://github.com/davidmarkclements/0x>)
to generate a flame graph for our Node.js application.

Weâ€™ll be using the `0x` tool to profile our server and generate a flame graph. Weâ€™ll also
need to use the `autocannon` tool to generate a load on our application

Now, instead of starting our server with the node binary, we need to start it with the `0x`
executable. If we open the `package.json` file, weâ€™ll see that the `npm start` script is
`node ./bin/www`. We need to substitute the `node` binary in the terminal command with `0x`:

```Bash
$ pnpm dlx 0x ./bin/www
ğŸ”¥  Flamegraph generated in
file:///... /flamegraph-app/8256.0x/flamegraph.html

```

Now, we need to generate some load on the server. In a new terminal window, use the
`autocannon` benchmarking tool to generate a load by running the following command:

```Bash
$ pnpm dlx autocannon --connections 100 http://localhost:3000
```

Expect to see the following output when the autocannon load test has been completed:

```Bash
Running 10s test @ http://localhost:3000/
100 connections


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stat    â”‚ 2.5%  â”‚ 50%    â”‚ 97.5%  â”‚ 99%     â”‚ Avg       â”‚ Stdev    â”‚ Max     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Latency â”‚ 63 ms â”‚ 183 ms â”‚ 746 ms â”‚ 4508 ms â”‚ 277.82 ms â”‚ 696.8 ms â”‚ 8219 ms â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stat      â”‚ 1%      â”‚ 2.5%    â”‚ 50%    â”‚ 97.5%  â”‚ Avg    â”‚ Stdev   â”‚ Min     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Req/Sec   â”‚ 61      â”‚ 61      â”‚ 348    â”‚ 560    â”‚ 357    â”‚ 135.13  â”‚ 61      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Bytes/Sec â”‚ 24.4 kB â”‚ 24.4 kB â”‚ 139 kB â”‚ 223 kB â”‚ 142 kB â”‚ 53.9 kB â”‚ 24.3 kB â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Req/Bytes counts sampled once per second.
# of samples: 10

4k requests in 10.05s, 1.42 MB read
```

Note that in this load test, our server was handling `357` requests per second on average.

Return to the terminal window where the server was started and press Ctrl + C. This will stop
the server. At this point, 0x will convert the captured stacks into a flame graph.

Expect to see the following output after pressing Ctrl + C. This output details the location where
`0x` has generated the flame graph. Observe that the `0x` tool has created a directory named
`8256.0x`, where `8256` is the process identifier (PID) of the server process:

Open the `flamegraph.html` file thatâ€™s been generated in the `flamegraph-app` directory
with Google Chrome. You can do this by copying the path to the flame graph and pasting it into
the Google Chrome address bar. Expect to see the generated flame graph and some controls.

Observe that the bars in the flame graph are of different shades. A darker (redder) shade
indicates a hot code path.

**Important note:**
Each generated flame graph may be slightly different, even when running the same load test.
The flame graph thatâ€™s generated on your device is likely to look different from the output shown
here. This is due to the non-deterministic nature of the profiling process, which may
have subtle impacts on the flame graphâ€™s output. However, generally, the flame graphâ€™s overall
results and bottlenecks are identified consistently.

Identify one of the darker frames. In the example flame graph, we can see that the
`readFileSync()` frame method has a darker shade â€“ indicating that that function has
spent a relatively large amount of time on the CPU:

Click on the darker frame. If itâ€™s difficult to identify the frame, you can enter `readFileSync`
into the search bar (top right), after which the frame will be highlighted. Upon clicking on the
frame, `0x` will expand the parent and child stacks of the selected frame

From the drilled-down view, we can see the hot code path. From the flame graph, we can make
an educated guess about which functions it would be worthwhile to invest time in optimizing.
In this case, we can see references to `handleTemplateCache()`.
Benchmarking HTTP requests, we learned how `pug` reloads a template for each request when
in development mode. This is the cause of this bottleneck. Letâ€™s change the application so that
it runs in production mode and see what the impact is on the load test results and flame graph.

Restart the Express.js server in production mode with the following command:
`$ NODE_ENV=production 0x ./bin/www`
Rerun the load test using the autocannon tool:
`$ autocannon --connections 100 http://localhost:3000`

```Bash
Running 10s test @ http://localhost:3000/
100 connections


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stat    â”‚ 2.5%  â”‚ 50%   â”‚ 97.5% â”‚ 99%    â”‚ Avg     â”‚ Stdev    â”‚ Max     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Latency â”‚ 20 ms â”‚ 35 ms â”‚ 94 ms â”‚ 165 ms â”‚ 43.5 ms â”‚ 90.37 ms â”‚ 2503 ms â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stat      â”‚ 1%     â”‚ 2.5%   â”‚ 50%    â”‚ 97.5%   â”‚ Avg     â”‚ Stdev    â”‚ Min    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Req/Sec   â”‚ 602    â”‚ 602    â”‚ 2,351  â”‚ 3,953   â”‚ 2,272.6 â”‚ 1,046.05 â”‚ 602    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Bytes/Sec â”‚ 240 kB â”‚ 240 kB â”‚ 938 kB â”‚ 1.58 MB â”‚ 907 kB  â”‚ 417 kB   â”‚ 240 kB â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Req/Bytes counts sampled once per second.
# of samples: 10

23k requests in 10.05s, 9.07 MB read
```

From the results of the load test, we can see that our server is handling more requests per
second. In this run, our load test reported that our server handled an average of around `2272`
requests per second, up from around `357` before we changed the Express.js server so that it
runs in production mode.

As before, once the `autocannon` load test is complete, stop your server using _Ctrl + C_. A new
flame graph will be generated. Open the new flame graph in your browser and observe that
the new flame graph is a different shape from the first. Observe that the second flame graph
highlights a different set of darker frames. This is because weâ€™ve resolved our first bottleneck.
Hot code paths are relative. Despite having increased the performance of our application, the
flame graph will identify the next set of hot code paths.

With that, weâ€™ve used `0x` to generate a flame graph, which has enabled us to identify a bottleneck in
our application.

## How it worksâ€¦

We used the `0x` tool to profile and generate a flame graph for our application. Our
application was a small, generated Express.js web server. The `autocannon` tool was used to add load
to our web server so that we could produce a flame graph thatâ€™s representative of a production workload.

To use the `0x` tool, we had to start our server with `0x`. When we start an application with `0x`, two
processes are started.

The first process uses the Node.js binary, `node`, to start our program. When `0x` starts the node
process, it passes the `--perf-basic-prof` command-line flag to the process. This command-
line flag allows C++ V8 function calls to be mapped to the corresponding JavaScript function calls.

The second process starts the local systemâ€™s stack tracing tool. On Linux, the `perf` tool will be
invoked, whereas on macOS and SmartOS, the `dtrace` tool will be invoked. These tools capture the
underlying C-level function calls.

The underlying system stack tracing tool will take samples. A **sample** is a snapshot of all the functions
being executed by the CPU at the time the sample was taken, which will also record the parent
function calls.

The sampled stacks are grouped based on the call hierarchy, grouping the parent and child function
calls together. These groups are whatâ€™s known as a **flame**, hence the name **flame graph**. The same
function may appear in multiple flames.

Each line in a flame is known as a frame. A **frame** represents a function call. The width of the frame
corresponds to the amount of time that that function was observed by the profiler on the CPU. The
time representation of each frame aggregates the time that all child functions take as well, hence the
triangular or _flame_ shape of the graph.

Darker (redder) frames indicate that a function has spent more time at the top of the stack relative
to the other functions. This means that this function is spending a lot of time on the CPU, which
indicates a potential bottleneck.

**Important note:**
Chrome DevTools can also be used to profile the CPU, which can help identify bottlenecks. Using
the `--inspect` command-line flag, the Node.js process can be debugged and profiled using
Chrome DevTools.
